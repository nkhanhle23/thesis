{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will execute the pre-processing steps by removing noises in text. The clean version will be saved for 1) lemmatization for TF-IDF, 2) tokenizing for FinBERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import os\n",
    "import time\n",
    "import multiprocessing\n",
    "import warnings\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "# Data handling and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Natural Language Processing (NLP)\n",
    "import spacy\n",
    "from gensim.parsing.preprocessing import STOPWORDS, strip_punctuation, strip_multiple_whitespaces, remove_stopwords, strip_short\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel#, Word2Vec\n",
    "#from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.compose import ColumnTransformer\n",
    "from sklearn.exceptions import FitFailedWarning\n",
    "from sklearn.metrics import make_scorer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RevenueFromContractWithCustomerExcludingAssessedTax_train.csv',\n",
       " 'NetCashProvidedByUsedInInvestingActivities_train.csv',\n",
       " 'NetCashProvidedByUsedInFinancingActivities_train.csv',\n",
       " 'EBIT_train.csv',\n",
       " 'NetIncomeLoss_train.csv',\n",
       " 'EarningsPerShareDiluted_train.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all training data files\n",
    "train_path = '../datasets/distilbert_data/train/'\n",
    "train_files = [file for file in os.listdir(train_path) if file.endswith('train.csv')]\n",
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EBIT_test.csv',\n",
       " 'NetCashProvidedByUsedInFinancingActivities_test.csv',\n",
       " 'RevenueFromContractWithCustomerExcludingAssessedTax_test.csv',\n",
       " 'EarningsPerShareDiluted_test.csv',\n",
       " 'NetIncomeLoss_test.csv',\n",
       " 'NetCashProvidedByUsedInInvestingActivities_test.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all test data files\n",
    "test_path = '../datasets/distilbert_data/test/'\n",
    "test_files = [file for file in os.listdir(test_path) if file.endswith('test.csv')]\n",
    "test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RevenueFromContractWithCustomerExcludingAssessedTax_train.csv',\n",
       " 'NetCashProvidedByUsedInInvestingActivities_train.csv',\n",
       " 'NetCashProvidedByUsedInFinancingActivities_train.csv',\n",
       " 'EBIT_train.csv',\n",
       " 'NetIncomeLoss_train.csv',\n",
       " 'EarningsPerShareDiluted_train.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all training data files\n",
    "# fb = finbert\n",
    "fb_train_path = '../datasets/finbert_data/train/'\n",
    "fb_train_files = [file for file in os.listdir(fb_train_path) if file.endswith('train.csv')]\n",
    "fb_train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EBIT_test.csv',\n",
       " 'NetCashProvidedByUsedInFinancingActivities_test.csv',\n",
       " 'RevenueFromContractWithCustomerExcludingAssessedTax_test.csv',\n",
       " 'EarningsPerShareDiluted_test.csv',\n",
       " 'NetIncomeLoss_test.csv',\n",
       " 'NetCashProvidedByUsedInInvestingActivities_test.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all test data files\n",
    "fb_test_path = '../datasets/finbert_data/test/'\n",
    "fb_test_files = [file for file in os.listdir(fb_test_path) if file.endswith('test.csv')]\n",
    "fb_test_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings (i.e SettingWithCopyWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "    \n",
    "# Function to preprocess and lemmatize text\n",
    "def preprocess_text(df, target_column):\n",
    "    \n",
    "    # Remove punctuation\n",
    "    df['text'] = df['text'].apply(lambda x: strip_punctuation(x))\n",
    "\n",
    "    # Remove multiple whitespaces\n",
    "    df['text'] = df['text'].apply(lambda x: strip_multiple_whitespaces(x))\n",
    "\n",
    "    # Transform to lowercase\n",
    "    df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "\n",
    "    # Remove stopwords\n",
    "    df['text'] = df['text'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "    # Remove short words\n",
    "    df['text'] = df['text'].apply(lambda x: strip_short(x))\n",
    "\n",
    "    \n",
    "    # Prepare data for training\n",
    "    df = df[['text', target_column]]\n",
    "    df.columns = ['text', 'target'] # Rename columns\n",
    "    \n",
    "    # Drop duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Drop rows with missing values\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing RevenueFromContractWithCustomerExcludingAssessedTax_train.csv...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text       target\n",
      "0  management considers factors assessing expecte...   5429000000\n",
      "1  estimate fair values assets acquired acquisiti...   4147270000\n",
      "2  actual future taxable income differs estimates...   7651319000\n",
      "3  company expects subject incremental tax gilti ...   6820886000\n",
      "4  accumulated unrepatriated indian earnings repa...  16783000000\n",
      "-----------------------------------------\n",
      "Preprocessing NetCashProvidedByUsedInInvestingActivities_train.csv...\n",
      "                                                text        target\n",
      "0  fourth quarter 2018 expanded restructuring pla...  3.100000e+07\n",
      "1  given quality diversity underlying real estate... -7.340000e+08\n",
      "2  cash flow amounts reduced realized gains inves... -6.240000e+08\n",
      "3  plan expand network tesla stores galleries ser... -1.081085e+09\n",
      "4  given quality diversity underlying real estate... -3.611000e+09\n",
      "-----------------------------------------\n",
      "Preprocessing NetCashProvidedByUsedInFinancingActivities_train.csv...\n",
      "                                                text        target\n",
      "0  pension expense vary range outcomes material e... -3.624000e+09\n",
      "1  order achieve level cost savings estimate rela...  8.830000e+08\n",
      "2  long term liabilities primarily include amount... -3.860000e+09\n",
      "3  annual cas cost planned cash funding years rec...  4.277000e+09\n",
      "4  commitments associated letters credit financin... -9.060000e+08\n",
      "-----------------------------------------\n",
      "Preprocessing EBIT_train.csv...\n",
      "                                                text        target\n",
      "0  estimating future taxable income develop assum...  5.488400e+11\n",
      "1  outlook looking ahead fiscal 2017 optimistic p...  4.624000e+09\n",
      "2  deferred tax assets net allowances expected re...  2.287600e+10\n",
      "3  anticipate cost savings realized workforce red...  2.564436e+10\n",
      "4  comparative purposes estimate operating income...  6.515398e+09\n",
      "-----------------------------------------\n",
      "Preprocessing NetIncomeLoss_train.csv...\n",
      "                                                text        target\n",
      "0  result exposed risks negatively affect sales p...  2.428000e+09\n",
      "1  foreign currency exchange contracts accounted ...  4.169000e+09\n",
      "2  review policies circumstances projected profit...  4.847000e+09\n",
      "3  forward looking statements include information...  3.149630e+08\n",
      "4  complexities able accurately forecast revenue ...  4.927000e+08\n",
      "-----------------------------------------\n",
      "Preprocessing EarningsPerShareDiluted_train.csv...\n",
      "                                                text  target\n",
      "0  factors cause dilution earnings share decrease...    3.04\n",
      "1  factors cause dilution earnings share iff decr...    4.00\n",
      "2  fiscal 2018 outlook optimistic prospects growt...    1.48\n",
      "3  excluding favorable currency impact prevailing...    3.88\n",
      "4  factors cause dilution earnings share decrease...    5.48\n",
      "-----------------------------------------\n",
      "-------------------------// DONE WITH TRAINING DATA //-------------------------\n",
      "Preprocessing EBIT_test.csv...\n",
      "                                                text        target\n",
      "0  prolonged substantial decline sustained market...  2.287600e+10\n",
      "1  continue opportunities margin expansion includ...  2.889530e+10\n",
      "2  estimate deferred tax assets liabilities incom...  1.443870e+10\n",
      "3  operating income operating income percentage s...  2.516312e+10\n",
      "4  projecting future taxable income begin histori...  9.743500e+09\n",
      "Preprocessing NetCashProvidedByUsedInFinancingActivities_test.csv...\n",
      "                                                text        target\n",
      "0  commitments credit committed facilities expire... -1.375000e+09\n",
      "1  difficult predict future pension costs changes... -8.058000e+08\n",
      "2  global pension prb cash funding requirements e... -3.860000e+09\n",
      "3  difficult predict future pension costs changes...  4.434000e+08\n",
      "4  pension expense vary range outcomes material e... -4.077000e+09\n",
      "Preprocessing RevenueFromContractWithCustomerExcludingAssessedTax_test.csv...\n",
      "                                                text       target\n",
      "0  partnership requested plan request ruling irs ...   7800800000\n",
      "1  addition excluding impact discrete items expec...  33705000000\n",
      "2  company net proceeds 2026 euro notes repay amo...   8598900000\n",
      "3  goodwill valuations performing quantitative as...  21253000000\n",
      "4  company records income taxes based estimated i...  12261000000\n",
      "Preprocessing EarningsPerShareDiluted_test.csv...\n",
      "                                                text  target\n",
      "0  year ended november 2018 percentage change rep...    2.62\n",
      "1  acquisitions strategic investments difficult t...    3.45\n",
      "2  adjusted diluted earnings share excluding esti...    3.72\n",
      "3  income income taxes reconciled adjusted income...    3.27\n",
      "4  expect adjusted diluted earnings share 2018 gr...    3.72\n",
      "Preprocessing NetIncomeLoss_test.csv...\n",
      "                                                text        target\n",
      "0  critical estimates valuing certain intangible ... -2.030000e+08\n",
      "1  order remain competitive continue increase rev...  2.017530e+08\n",
      "2  competitors large established greater market s...  5.642000e+09\n",
      "3  decrease expense year ended december 2015 prim...  5.660000e+08\n",
      "4  expecting strong margins foreseeable future 20...  2.465036e+09\n",
      "Preprocessing NetCashProvidedByUsedInInvestingActivities_test.csv...\n",
      "                                                text        target\n",
      "0  liquidity needs estimate liquidity needs month... -1.186000e+09\n",
      "1  liquidity needs include anticipated requiremen... -2.611000e+09\n",
      "2  joint venture expect cash investment 840 milli... -6.678000e+08\n",
      "3  company extensive non operations significant c... -1.228800e+09\n",
      "4  liquidity needs estimate liquidity needs month... -3.570000e+09\n",
      "-------------------------// DONE WITH TEST DATA //-------------------------\n"
     ]
    }
   ],
   "source": [
    "# Preprocess all training and test data\n",
    "for file in train_files:\n",
    "    print(f'Preprocessing {file}...')\n",
    "    df = pd.read_csv(os.path.join(train_path, file))\n",
    "    df = preprocess_text(df, 'val')\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"-----------------------------------------\")\n",
    "\n",
    "    # Save preprocessed data\n",
    "    df.to_csv(train_path + file.split('.')[0] + \"__text_clean.csv\" , index=False)\n",
    "    \n",
    "print(\"-------------------------// DONE WITH TRAINING DATA //-------------------------\")\n",
    "\n",
    "for file in test_files:\n",
    "    print(f'Preprocessing {file}...')\n",
    "    df = pd.read_csv(os.path.join(test_path, file))\n",
    "    df = preprocess_text(df, 'val')\n",
    "    print(df.head())\n",
    "\n",
    "    # Save preprocessed data\n",
    "    df.to_csv(test_path + file.split('.')[0] + \"__text_clean.csv\" , index=False)\n",
    "\n",
    "print(\"-------------------------// DONE WITH TEST DATA //-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing RevenueFromContractWithCustomerExcludingAssessedTax_train.csv...\n",
      "                                                text       target\n",
      "0  total net proceeds approximately 089 million i...   4147270000\n",
      "1  regularly review recoverability deferred tax a...   2408200000\n",
      "2  given period change deferred gain included net...   1360000000\n",
      "3  income approach estimate projected future cash...  16783000000\n",
      "4  income approach based projected future cash fl...    900465000\n",
      "-----------------------------------------\n",
      "Preprocessing NetCashProvidedByUsedInInvestingActivities_train.csv...\n",
      "                                                text        target\n",
      "0  account contracted condominium sales percentag... -8.379220e+08\n",
      "1  cash flow amounts reduced realized gains inves... -6.240000e+08\n",
      "2  cash flow amounts reduced realized gains inves... -5.580000e+08\n",
      "3  company extensive non operations significant c... -1.612700e+09\n",
      "4  cash flow amounts reduced realized gains inves... -4.560000e+08\n",
      "-----------------------------------------\n",
      "Preprocessing NetCashProvidedByUsedInFinancingActivities_train.csv...\n",
      "                                                text        target\n",
      "0  commitments credit committed facilities expire... -1.375000e+09\n",
      "1  estimate related cash funding payments billion...  8.830000e+08\n",
      "2  commitments credit committed facilities expire... -2.801000e+09\n",
      "3  annual cas cost planned cash funding years rec...  4.277000e+09\n",
      "4  expect contributions approximately million fis... -1.116528e+09\n",
      "-----------------------------------------\n",
      "Preprocessing EBIT_train.csv...\n",
      "                                                text        target\n",
      "0  projecting future taxable income begin histori...  9.743500e+09\n",
      "1  focused approach expect fiscal 2017 organic ne...  2.856060e+10\n",
      "2  actual future results developments conform exp...  2.122800e+10\n",
      "3  currently estimate net effect applying provisi...  7.391458e+09\n",
      "4  underlying operating income declined adjusting...  5.499000e+09\n",
      "-----------------------------------------\n",
      "Preprocessing NetIncomeLoss_train.csv...\n",
      "                                                text        target\n",
      "0  factors result impairment include limited sign...  4.857000e+09\n",
      "1  current foreign exchange rate assumptions anti...  3.770310e+08\n",
      "2  valuation deferred tax assets requires judgmen...  5.900000e+08\n",
      "3  additional assumptions included compensation c...  1.454490e+08\n",
      "4  required record significant charges earnings d...  3.983550e+08\n",
      "-----------------------------------------\n",
      "Preprocessing EarningsPerShareDiluted_train.csv...\n",
      "                                                text  target\n",
      "0  factors cause dilution earnings share decrease...    3.04\n",
      "1  factors cause dilution earnings share iff decr...    4.00\n",
      "2  fiscal 2018 outlook optimistic prospects growt...    1.48\n",
      "3  excluding favorable currency impact prevailing...    3.88\n",
      "4  factors cause dilution earnings share decrease...    2.53\n",
      "-----------------------------------------\n",
      "-------------------------// DONE WITH TRAINING DATA //-------------------------\n",
      "Preprocessing EBIT_test.csv...\n",
      "                                                text        target\n",
      "0  estimating future cash flows rely internally g...  1.363500e+10\n",
      "1  estimate completion adjustments following impa...  1.251000e+10\n",
      "2  operating income operating income percentage s...  2.171957e+10\n",
      "3  2019 expect carrier consolidation driven churn...  1.273600e+09\n",
      "4  adjustments estimated total costs completion e...  1.251000e+10\n",
      "Preprocessing NetCashProvidedByUsedInFinancingActivities_test.csv...\n",
      "                                                text        target\n",
      "0  global pension prb cash funding requirements e... -3.860000e+09\n",
      "1  expect contributions approximately million fis... -9.143290e+08\n",
      "2  difficult predict future pension costs changes...  4.434000e+08\n",
      "3  long term liabilities primarily include amount... -3.860000e+09\n",
      "4  pension expense vary range outcomes material e... -4.077000e+09\n",
      "Preprocessing RevenueFromContractWithCustomerExcludingAssessedTax_test.csv...\n",
      "                                                text       target\n",
      "0  income approach reflects discounting future ca...  25110000000\n",
      "1  floating rate securities generally subject rat...   8650000000\n",
      "2  based 2021 exchange rates operating results do...   9107900000\n",
      "3  fiscal 2019 outlook optimistic prospects growt...   3324000000\n",
      "4  anticipated proceeds current carrying asset op...   2883673000\n",
      "Preprocessing EarningsPerShareDiluted_test.csv...\n",
      "                                                text  target\n",
      "0        diluted earnings share 2021 projected range    2.78\n",
      "1  acquisitions strategic investments difficult t...    3.45\n",
      "2  adjusted diluted earnings share excluding esti...    3.72\n",
      "3  2017 ppg annual report form income income taxe...    6.18\n",
      "4  expect adjusted diluted earnings share 2018 gr...    3.72\n",
      "Preprocessing NetIncomeLoss_test.csv...\n",
      "                                                text        target\n",
      "0  compete successfully environment continue inve...  4.050310e+08\n",
      "1  carrying value assets exceeds current estimate...  6.317000e+08\n",
      "2  november 2016 lennar homebuilding debt total c...  9.118440e+08\n",
      "3  estimated gross profits investment oriented pr... -8.490000e+08\n",
      "4  furthermore continue joint venture strategic i...  2.779000e+09\n",
      "Preprocessing NetCashProvidedByUsedInInvestingActivities_test.csv...\n",
      "                                                text        target\n",
      "0  liquidity needs estimate liquidity needs month... -2.611000e+09\n",
      "1  expect satisfy obligations cash generated oper... -2.611000e+09\n",
      "2  joint venture expect cash investment 840 milli... -6.678000e+08\n",
      "3  given quality diversity underlying real estate... -7.340000e+08\n",
      "4  liquidity needs estimate liquidity needs month... -3.875000e+09\n",
      "-------------------------// DONE WITH TEST DATA //-------------------------\n"
     ]
    }
   ],
   "source": [
    "# Preprocess all training and test data\n",
    "for file in fb_train_files:\n",
    "    print(f'Preprocessing {file}...')\n",
    "    df = pd.read_csv(os.path.join(fb_train_path, file))\n",
    "    df = preprocess_text(df, 'val')\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"-----------------------------------------\")\n",
    "\n",
    "    # Save preprocessed data\n",
    "    df.to_csv(fb_train_path + file.split('.')[0] + \"__text_clean.csv\" , index=False)\n",
    "    \n",
    "print(\"-------------------------// DONE WITH TRAINING DATA //-------------------------\")\n",
    "\n",
    "for file in fb_test_files:\n",
    "    print(f'Preprocessing {file}...')\n",
    "    df = pd.read_csv(os.path.join(fb_test_path, file))\n",
    "    df = preprocess_text(df, 'val')\n",
    "    print(df.head())\n",
    "\n",
    "    # Save preprocessed data\n",
    "    df.to_csv(fb_test_path + file.split('.')[0] + \"__text_clean.csv\" , index=False)\n",
    "\n",
    "print(\"-------------------------// DONE WITH TEST DATA //-------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train random forest regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NetIncomeLoss_train_preprocessed.csv',\n",
       " 'RevenueFromContractWithCustomerExcludingAssessedTax_train_preprocessed.csv',\n",
       " 'NetCashProvidedByUsedInFinancingActivities_train_preprocessed.csv',\n",
       " 'NetCashProvidedByUsedInInvestingActivities_train_preprocessed.csv',\n",
       " 'EarningsPerShareDiluted_train_preprocessed.csv',\n",
       " 'EBIT_train_preprocessed.csv']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------START TRAINING MODEL FOR NetIncomeLoss--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress - TF-IDF:   0%|          | 0/243 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress - TF-IDF:   0%|          | 1/243 [12:36<50:51:46, 756.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for TF-IDF: {'rf__max_depth': None, 'rf__max_features': 'log2', 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 10, 'rf__n_estimators': 500}\n",
      "Best negative RSE score for TF-IDF: -0.6334869727396837\n",
      "Time taken for TF-IDF grid search: 756.6324696540833 seconds\n",
      "--------------DONE TRAINING MODEL FOR NetIncomeLoss--------------\n",
      "\n",
      "\n",
      "--------------START TRAINING MODEL FOR RevenueFromContractWithCustomerExcludingAssessedTax--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress - TF-IDF:   0%|          | 1/243 [08:03<32:28:28, 483.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for TF-IDF: {'rf__max_depth': None, 'rf__max_features': 'log2', 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 2, 'rf__n_estimators': 500}\n",
      "Best negative RSE score for TF-IDF: -0.4792336024566242\n",
      "Time taken for TF-IDF grid search: 483.0873520374298 seconds\n",
      "--------------DONE TRAINING MODEL FOR RevenueFromContractWithCustomerExcludingAssessedTax--------------\n",
      "\n",
      "\n",
      "--------------START TRAINING MODEL FOR NetCashProvidedByUsedInFinancingActivities--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress - TF-IDF:   0%|          | 1/243 [00:15<1:04:04, 15.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for TF-IDF: {'rf__max_depth': None, 'rf__max_features': 1.0, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 2, 'rf__n_estimators': 100}\n",
      "Best negative RSE score for TF-IDF: -inf\n",
      "Time taken for TF-IDF grid search: 15.88652753829956 seconds\n",
      "--------------DONE TRAINING MODEL FOR NetCashProvidedByUsedInFinancingActivities--------------\n",
      "\n",
      "\n",
      "--------------START TRAINING MODEL FOR NetCashProvidedByUsedInInvestingActivities--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress - TF-IDF:   0%|          | 1/243 [00:16<1:07:45, 16.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for TF-IDF: {'rf__max_depth': 8, 'rf__max_features': 'sqrt', 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 2, 'rf__n_estimators': 100}\n",
      "Best negative RSE score for TF-IDF: -1.375560567298534\n",
      "Time taken for TF-IDF grid search: 16.797523975372314 seconds\n",
      "--------------DONE TRAINING MODEL FOR NetCashProvidedByUsedInInvestingActivities--------------\n",
      "\n",
      "\n",
      "--------------START TRAINING MODEL FOR EarningsPerShareDiluted--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress - TF-IDF:   0%|          | 1/243 [00:33<2:14:14, 33.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for TF-IDF: {'rf__max_depth': None, 'rf__max_features': 'log2', 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 10, 'rf__n_estimators': 200}\n",
      "Best negative RSE score for TF-IDF: -0.8818554951887793\n",
      "Time taken for TF-IDF grid search: 33.27862811088562 seconds\n",
      "--------------DONE TRAINING MODEL FOR EarningsPerShareDiluted--------------\n",
      "\n",
      "\n",
      "--------------START TRAINING MODEL FOR EBIT--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress - TF-IDF:   0%|          | 1/243 [00:22<1:29:02, 22.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for TF-IDF: {'rf__max_depth': None, 'rf__max_features': 'sqrt', 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 2, 'rf__n_estimators': 200}\n",
      "Best negative RSE score for TF-IDF: -0.7814153522870717\n",
      "Time taken for TF-IDF grid search: 22.075989484786987 seconds\n",
      "--------------DONE TRAINING MODEL FOR EBIT--------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obmitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define the RSE scoring function\n",
    "def rse_scorer(y_true, y_pred):\n",
    "    true_mean = np.mean(y_true)\n",
    "    squared_error_num = np.sum(np.square(y_true - y_pred))\n",
    "    squared_error_den = np.sum(np.square(y_true - true_mean))\n",
    "    rse_loss = squared_error_num / squared_error_den\n",
    "    return -rse_loss  # Note the negative sign since GridSearchCV maximizes the score\n",
    "\n",
    "\n",
    "# Custom transformer for Doc2Vec\n",
    "class Doc2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vector_size=100, window=5, min_count=None, workers=None):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers if workers is not None else multiprocessing.cpu_count()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.documents = [TaggedDocument(doc.split(), [i]) for i, doc in enumerate(X)]\n",
    "        \n",
    "        # Set min_count to 5% of the document count if not explicitly provided\n",
    "        if self.min_count is None:\n",
    "            self.min_count = round(len(self.documents) * 0.05)\n",
    "            \n",
    "        self.model = Doc2Vec(vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=self.workers)\n",
    "        self.model.build_vocab(self.documents)\n",
    "        self.model.train(self.documents, total_examples=self.model.corpus_count, epochs=self.model.epochs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([self.model.infer_vector(doc.split()) for doc in X])\n",
    "\n",
    "\n",
    "\n",
    "# Function to train a random forest regressor on the preprocessed data\n",
    "def train_random_forest_regressor(df, target_column, random_state=42):\n",
    "    # Split the data into training and testing sets\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=random_state)\n",
    "\n",
    "    # Define the pipeline\n",
    "    pipeline_tfidf = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_df=0.95, min_df=round(len(train_df) * 0.05))),\n",
    "        ('rf', RandomForestRegressor(random_state=random_state))\n",
    "    ])\n",
    "\n",
    "    pipeline_doc2vec = Pipeline([\n",
    "        ('doc2vec', Doc2VecTransformer()),\n",
    "        ('rf', RandomForestRegressor(random_state=random_state))\n",
    "    ])\n",
    "\n",
    "    # Define the parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'rf__n_estimators': [100, 200, 500],\n",
    "        'rf__max_features': [1.0, 'sqrt', 'log2'],\n",
    "        'rf__max_depth': [None, 4, 8],\n",
    "        'rf__min_samples_split': [2, 10, 20],\n",
    "        'rf__min_samples_leaf': [1, 5, 10]\n",
    "    }\n",
    "    \n",
    "    # Define the vector sizes to try for Doc2Vec\n",
    "    doc2vec_vector_sizes = [50, 100, 200]\n",
    "    \n",
    "    # Add timer to record time\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=FitFailedWarning)\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"One or more of the test scores are non-finite\")\n",
    "\n",
    "        # Perform grid search for TF-IDF\n",
    "        with tqdm(total=len(param_grid['rf__n_estimators']) * len(param_grid['rf__max_features']) *\n",
    "                  len(param_grid['rf__max_depth']) * len(param_grid['rf__min_samples_split']) *\n",
    "                  len(param_grid['rf__min_samples_leaf']), desc='Grid Search Progress - TF-IDF') as pbar:\n",
    "\n",
    "            start_time_tfidf = time.time()  # Record the start time\n",
    "\n",
    "            grid_search_tfidf = GridSearchCV(pipeline_tfidf, param_grid, cv=5, scoring=make_scorer(rse_scorer), n_jobs=-1)\n",
    "            grid_search_tfidf.fit(train_df['text'], train_df[target_column])\n",
    "\n",
    "            end_time_tfidf = time.time()  # Record the end time\n",
    "            elapsed_time_tfidf = end_time_tfidf - start_time_tfidf  # Calculate the elapsed time\n",
    "\n",
    "            pbar.update(1)  # Update the progress bar\n",
    "\n",
    "        # Access the best parameters and performance metrics for TF-IDF\n",
    "        print(f\"Best parameters for TF-IDF: {grid_search_tfidf.best_params_}\")\n",
    "        print(f\"Best negative RSE score for TF-IDF: {grid_search_tfidf.best_score_}\")\n",
    "        print(f\"Time taken for TF-IDF grid search: {elapsed_time_tfidf} seconds\")\n",
    "        print(\"------------------------------------------------------------\")\n",
    "\n",
    "        # Perform grid search for Doc2Vec with different vector sizes\n",
    "        best_doc2vec_vector_size = None\n",
    "        best_doc2vec_score = float('-inf')  # Initialize with negative infinity\n",
    "\n",
    "        start_time_doc2vec = time.time()  # Record the start time\n",
    "        for vector_size in tqdm(doc2vec_vector_sizes, desc='Vector Size Progress - Doc2Vec'):\n",
    "            pipeline_doc2vec.set_params(doc2vec__vector_size=vector_size)\n",
    "            grid_search_doc2vec = GridSearchCV(pipeline_doc2vec, param_grid, cv=5, scoring=make_scorer(rse_scorer), n_jobs=-1)\n",
    "            grid_search_doc2vec.fit(train_df['text'], train_df[target_column])\n",
    "\n",
    "            # Check if the current vector size has a better score than the previous best\n",
    "            if grid_search_doc2vec.best_score_ > best_doc2vec_score:\n",
    "                best_doc2vec_score = grid_search_doc2vec.best_score_\n",
    "                best_doc2vec_vector_size = vector_size\n",
    "                best_doc2vec_params = grid_search_doc2vec.best_params_\n",
    "                best_doc2vec_model = grid_search_doc2vec\n",
    "                \n",
    "        end_time_doc2vec = time.time()\n",
    "        elapsed_time_doc2vec = end_time_doc2vec - start_time_doc2vec\n",
    "        \n",
    "        \n",
    "        print(f\"Best parameters for Doc2Vec (vector_size={best_doc2vec_vector_size}): {best_doc2vec_params}\")\n",
    "        print(f\"Best negative RSE score for Doc2Vec: {best_doc2vec_score} (vector_size={best_doc2vec_vector_size})\")\n",
    "        print(f\"Time taken for Doc2Vec grid search: {elapsed_time_doc2vec} seconds\")\n",
    "        print(\"------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    # Evaluate the models on the test set\n",
    "    y_pred_tfidf = grid_search_tfidf.predict(test_df['text'])\n",
    "    y_pred_d2v = grid_search_doc2vec.predict(test_df['text'])\n",
    "    # Function to calculate the relative squared error\n",
    "    # Pros of RSE:\n",
    "    # - It is scale independent --> can be used to compare models with different scales\n",
    "    # - It is symmetric\n",
    "    # - It is easy to interpret --> below 1 means that the model is better than the baseline, above 1 means that the model is worse than the baseline\n",
    "    def rse(y_true, y_pred):\n",
    "        return -rse_scorer(y_true, y_pred)  # Reverse the sign of the RSE score to get the RSE\n",
    "    \n",
    "    # Train RSE\n",
    "    train_tfidf_score = -1 * grid_search_tfidf.best_score_\n",
    "    train_doc2vec_score = -1 * best_doc2vec_score\n",
    "    # Test RSE\n",
    "    test_tfidf_score = rse(test_df[target_column], y_pred_tfidf)\n",
    "    test_doc2vec_score = rse(test_df[target_column], y_pred_d2v)\n",
    "    \n",
    "    # Create a dataframe with the results\n",
    "    results = pd.DataFrame({'Model': ['TFIDF', 'Doc2Vec'], \n",
    "                            'Train RSE': [train_tfidf_score, train_doc2vec_score],\n",
    "                            'Test RSE': [test_tfidf_score, test_doc2vec_score]\n",
    "                            })\n",
    "    print(results)\n",
    "    return grid_search_tfidf, best_doc2vec_model\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models on the test set\n",
    "    y_pred_tfidf = grid_search_tfidf.predict(test_df['text'])\n",
    "    y_pred_d2v = grid_search_doc2vec.predict(test_df['text'])\n",
    "    # Function to calculate the relative squared error\n",
    "    # Pros of RSE:\n",
    "    # - It is scale independent --> can be used to compare models with different scales\n",
    "    # - It is symmetric\n",
    "    # - It is easy to interpret --> below 1 means that the model is better than the baseline, above 1 means that the model is worse than the baseline\n",
    "    def rse(y_true, y_pred):\n",
    "        return -rse_scorer(y_true, y_pred)  # Reverse the sign of the RSE score to get the RSE\n",
    "    \n",
    "    # Train RSE\n",
    "    train_tfidf_score = -1 * grid_search_tfidf.best_score_\n",
    "    train_doc2vec_score = -1 * best_doc2vec_score\n",
    "    # Test RSE\n",
    "    test_tfidf_score = rse(test_df[target_column], y_pred_tfidf)\n",
    "    test_doc2vec_score = rse(test_df[target_column], y_pred_d2v)\n",
    "    \n",
    "    # Create a dataframe with the results\n",
    "    results = pd.DataFrame({'Model': ['TFIDF', 'Doc2Vec'], \n",
    "                            'Train RSE': [train_tfidf_score, train_doc2vec_score],\n",
    "                            'Test RSE': [test_tfidf_score, test_doc2vec_score]\n",
    "                            })\n",
    "    print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

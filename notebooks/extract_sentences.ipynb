{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.matcher import Matcher, PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file\n",
    "with open('datasets/EXTRACTED_FILINGS.json', 'r') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_1A</th>\n",
       "      <th>item_7</th>\n",
       "      <th>item_7A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The healthcare products distribution industry ...</td>\n",
       "      <td>Cautionary Note Regarding Forward-Looking Stat...</td>\n",
       "      <td>We are exposed to market risks, which include ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The healthcare products distribution industry ...</td>\n",
       "      <td>Cautionary Note Regarding Forward-Looking Stat...</td>\n",
       "      <td>We are exposed to market risks, which include ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The healthcare products distribution industry ...</td>\n",
       "      <td>Cautionary Note Regarding Forward-Looking Stat...</td>\n",
       "      <td>We are exposed to market risks, which include ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Declining economic conditions could adversely ...</td>\n",
       "      <td>Cautionary Note Regarding Forward-Looking Stat...</td>\n",
       "      <td>We are exposed to market risks, which include ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Declining economic conditions could adversely ...</td>\n",
       "      <td>Cautionary Note Regarding Forward-Looking Stat...</td>\n",
       "      <td>We are exposed to market risks, which include ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             item_1A  \\\n",
       "0  The healthcare products distribution industry ...   \n",
       "1  The healthcare products distribution industry ...   \n",
       "2  The healthcare products distribution industry ...   \n",
       "3  Declining economic conditions could adversely ...   \n",
       "4  Declining economic conditions could adversely ...   \n",
       "\n",
       "                                              item_7  \\\n",
       "0  Cautionary Note Regarding Forward-Looking Stat...   \n",
       "1  Cautionary Note Regarding Forward-Looking Stat...   \n",
       "2  Cautionary Note Regarding Forward-Looking Stat...   \n",
       "3  Cautionary Note Regarding Forward-Looking Stat...   \n",
       "4  Cautionary Note Regarding Forward-Looking Stat...   \n",
       "\n",
       "                                             item_7A  \n",
       "0  We are exposed to market risks, which include ...  \n",
       "1  We are exposed to market risks, which include ...  \n",
       "2  We are exposed to market risks, which include ...  \n",
       "3  We are exposed to market risks, which include ...  \n",
       "4  We are exposed to market risks, which include ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = df[['item_1A','item_7','item_7A']]\n",
    "content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_content = content.iloc[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Spacy to extract sentences from the text \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs rule-based matching on the text to extract sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy installation\n",
    "# !pip install -U spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import spacy\n",
    "# import Matcher\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finance'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "load_model = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "words = 'financing'\n",
    "\" \".join([token.lemma_ for token in load_model(words)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## newest version of extracting sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next steps: find out a way to speed up the process of extracting sentences + write a function to extract sentences from a list of texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old version, input is a dataframe with 3 columns (item_1A, item_7, item_7A)\n",
    "def extract_sentences_with_metrics(text_data):\n",
    "\n",
    "    # Load the spaCy language model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # Define custom spaCy matchers for financial metrics\n",
    "\n",
    "    # Revenue Matcher\n",
    "    revenue_matcher = Matcher(nlp.vocab)\n",
    "    revenue_matcher.add(\"revenue_match\", [[{'LOWER': {'IN': ['income', 'proceeds', 'takings', 'receipts', 'sales', 'turnover']}}]])\n",
    "\n",
    "    # Net Income Matcher\n",
    "    net_income_matcher = PhraseMatcher(nlp.vocab)\n",
    "    net_income_patterns = [nlp(text) for text in ('net income', 'profit', 'earnings', 'bottom line')]\n",
    "    net_income_matcher.add(\"net_income_match\", None, *net_income_patterns)\n",
    "\n",
    "    # EBIT Matcher\n",
    "    ebit_matcher = PhraseMatcher(nlp.vocab)\n",
    "    ebit_patterns = [nlp(text) for text in ('ebit', 'earnings before interest and taxes', 'operating profit', 'operating income')]\n",
    "    ebit_matcher.add(\"ebit_match\", None, *ebit_patterns)\n",
    "\n",
    "    # EPS Matcher\n",
    "    eps_matcher = PhraseMatcher(nlp.vocab)\n",
    "    eps_patterns = [nlp(text) for text in ('eps', 'earnings per share')]\n",
    "    eps_matcher.add(\"eps_match\", None, *eps_patterns)\n",
    "\n",
    "    # Cash Flow Matchers\n",
    "    cash_flow_matcher = Matcher(nlp.vocab)\n",
    "    operating_patterns = [\n",
    "        {'LOWER': {'in': ['cash', 'flow']}},\n",
    "        {'LEMMA': {'in': ['operate','operation']}},\n",
    "    ]\n",
    "\n",
    "    operating_patterns1 = [\n",
    "        {'LOWER': {'in': ['cffo','cfo']}},\n",
    "    ]\n",
    "\n",
    "    investing_patterns = [\n",
    "        {'LOWER': {'in': ['cash', 'flow']}},\n",
    "        {'LEMMA': {'in': ['invest','investment']}},\n",
    "    ]\n",
    "\n",
    "    financing_patterns = [\n",
    "        {'LOWER': {'in': ['cash', 'flow']}},\n",
    "        {'LEMMA': {'in': ['finance', 'funding']}},\n",
    "    ]\n",
    "\n",
    "\n",
    "    cash_flow_matcher.add(\"operating_match\", [operating_patterns])\n",
    "    cash_flow_matcher.add(\"operating_match1\", [operating_patterns1])\n",
    "    cash_flow_matcher.add(\"investing_match\", [investing_patterns])\n",
    "    cash_flow_matcher.add(\"financing_match\", [financing_patterns])\n",
    "\n",
    "    # Initialize lists to store categorized results for both financial metrics and cash flows\n",
    "    sentences_metrics = []\n",
    "\n",
    "    # Define a mapping of matchers to metric names\n",
    "    matcher_to_metric = {\n",
    "        revenue_matcher: \"Revenue\",\n",
    "        net_income_matcher: \"Net Income\",\n",
    "        ebit_matcher: \"EBIT\",\n",
    "        eps_matcher: \"EPS\",\n",
    "        cash_flow_matcher: \"Cash Flow\"\n",
    "    }\n",
    "\n",
    "    # Iterate through each column of the DataFrame\n",
    "    for column in text_data.columns:\n",
    "        # Iterate through each row in the column\n",
    "        for idx, text in enumerate(text_data[column]):\n",
    "            # Split the text into sentences\n",
    "            sentences = [sentence.text for sentence in nlp(text).sents]\n",
    "\n",
    "            # Iterate through each sentence\n",
    "            for sentence in sentences:\n",
    "                doc = nlp(sentence)\n",
    "\n",
    "                # Check for financial metrics matches\n",
    "                for matcher, metric_name in matcher_to_metric.items():\n",
    "                    matches = matcher(doc)\n",
    "                    if matches:\n",
    "                        for match_id, start, end in matches:\n",
    "                            if matcher == cash_flow_matcher:\n",
    "                                matches = matcher(doc)\n",
    "                                for match_id, start, end in matches:\n",
    "                                    if nlp.vocab.strings[match_id] == \"operating_match\":\n",
    "                                        sentences_metrics.append({'Sentence': sentence, 'Metric': 'Cash Flow (Operating)'})\n",
    "                                    elif nlp.vocab.strings[match_id] == \"operating_match1\":\n",
    "                                        sentences_metrics.append({'Sentence': sentence, 'Metric': 'Cash Flow (Operating)'})\n",
    "                                    elif nlp.vocab.strings[match_id] == \"investing_match\":\n",
    "                                        sentences_metrics.append({'Sentence': sentence, 'Metric': 'Cash Flow (Investing)'})\n",
    "                                    elif nlp.vocab.strings[match_id] == \"financing_match\":\n",
    "                                        sentences_metrics.append({'Sentence': sentence, 'Metric': 'Cash Flow (Financing)'})\n",
    "                                    \n",
    "                            else: sentences_metrics.append({'Sentence': sentence, 'Metric': metric_name})\n",
    "\n",
    "\n",
    "    # Create a DataFrame with combined results\n",
    "    sentences_metrics_df = pd.DataFrame(sentences_metrics)\n",
    "\n",
    "    # Print the resulting DataFrame\n",
    "    sentences_metrics_df.head()\n",
    "    return sentences_metrics_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_with_metrics(doc):\n",
    "\n",
    "    # Load the spaCy language model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # Define custom spaCy matchers for financial metrics\n",
    "\n",
    "    # Revenue Matcher\n",
    "    revenue_matcher = Matcher(nlp.vocab)\n",
    "    revenue_matcher.add(\"revenue_match\", [[{'LOWER': {'IN': ['income', 'proceeds', 'takings', 'receipts', 'sales', 'turnover']}}]])\n",
    "\n",
    "    # Net Income Matcher\n",
    "    net_income_matcher = PhraseMatcher(nlp.vocab)\n",
    "    net_income_patterns = [nlp(text) for text in ('net income', 'profit', 'earnings', 'bottom line')]\n",
    "    net_income_matcher.add(\"net_income_match\", None, *net_income_patterns)\n",
    "\n",
    "    # EBIT Matcher\n",
    "    ebit_matcher = PhraseMatcher(nlp.vocab)\n",
    "    ebit_patterns = [nlp(text) for text in ('ebit', 'earnings before interest and taxes', 'operating profit', 'operating income')]\n",
    "    ebit_matcher.add(\"ebit_match\", None, *ebit_patterns)\n",
    "\n",
    "    # EPS Matcher\n",
    "    eps_matcher = PhraseMatcher(nlp.vocab)\n",
    "    eps_patterns = [nlp(text) for text in ('eps', 'earnings per share')]\n",
    "    eps_matcher.add(\"eps_match\", None, *eps_patterns)\n",
    "\n",
    "    # Cash Flow Matchers\n",
    "    cash_flow_matcher = Matcher(nlp.vocab)\n",
    "    operating_patterns = [\n",
    "        {'LOWER': {'in': ['cash', 'flow']}},\n",
    "        {'LEMMA': {'in': ['operate','operation']}},\n",
    "    ]\n",
    "\n",
    "    operating_patterns1 = [\n",
    "        {'LOWER': {'in': ['cffo','cfo']}},\n",
    "    ]\n",
    "\n",
    "    investing_patterns = [\n",
    "        {'LOWER': {'in': ['cash', 'flow']}},\n",
    "        {'LEMMA': {'in': ['invest','investment']}},\n",
    "    ]\n",
    "\n",
    "    financing_patterns = [\n",
    "        {'LOWER': {'in': ['cash', 'flow']}},\n",
    "        {'LEMMA': {'in': ['finance', 'funding']}},\n",
    "    ]\n",
    "\n",
    "\n",
    "    cash_flow_matcher.add(\"operating_match\", [operating_patterns])\n",
    "    cash_flow_matcher.add(\"operating_match1\", [operating_patterns1])\n",
    "    cash_flow_matcher.add(\"investing_match\", [investing_patterns])\n",
    "    cash_flow_matcher.add(\"financing_match\", [financing_patterns])\n",
    "\n",
    "    # Initialize lists to store categorized results and \n",
    "    sentences_metrics = []\n",
    "\n",
    "    # Define a mapping of matchers to metric names\n",
    "    matcher_to_metric = {\n",
    "        revenue_matcher: \"Revenue\",\n",
    "        net_income_matcher: \"Net Income\",\n",
    "        ebit_matcher: \"EBIT\",\n",
    "        eps_matcher: \"EPS\",\n",
    "        cash_flow_matcher: \"Cash Flow\"\n",
    "    }\n",
    "\n",
    "    \n",
    "        # Iterate through each row in the column\n",
    "        for idx, text in enumerate(text_data[column]):\n",
    "            # Split the text into sentences\n",
    "            sentences = [sentence.text for sentence in nlp(text).sents]\n",
    "\n",
    "            # Iterate through each sentence\n",
    "            for sentence in sentences:\n",
    "                doc = nlp(sentence)\n",
    "\n",
    "                # Check for financial metrics matches\n",
    "                for matcher, metric_name in matcher_to_metric.items():\n",
    "                    matches = matcher(doc)\n",
    "                    if matches:\n",
    "                        for match_id, start, end in matches:\n",
    "                            if matcher == cash_flow_matcher:\n",
    "                                matches = matcher(doc)\n",
    "                                for match_id, start, end in matches:\n",
    "                                    if nlp.vocab.strings[match_id] == \"operating_match\":\n",
    "                                        sentences_metrics.append({'Sentence': sentence, 'Metric': 'Cash Flow (Operating)'})\n",
    "                                    elif nlp.vocab.strings[match_id] == \"operating_match1\":\n",
    "                                        sentences_metrics.append({'Sentence': sentence, 'Metric': 'Cash Flow (Operating)'})\n",
    "                                    elif nlp.vocab.strings[match_id] == \"investing_match\":\n",
    "                                        sentences_metrics.append({'Sentence': sentence, 'Metric': 'Cash Flow (Investing)'})\n",
    "                                    elif nlp.vocab.strings[match_id] == \"financing_match\":\n",
    "                                        sentences_metrics.append({'Sentence': sentence, 'Metric': 'Cash Flow (Financing)'})\n",
    "                                    \n",
    "                            else: sentences_metrics.append({'Sentence': sentence, 'Metric': metric_name})\n",
    "\n",
    "\n",
    "    # Create a DataFrame with combined results\n",
    "    sentences_metrics_df = pd.DataFrame(sentences_metrics)\n",
    "\n",
    "    # Print the resulting DataFrame\n",
    "    sentences_metrics_df.head()\n",
    "    return sentences_metrics_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_metrics_df = extract_sentences_with_metrics(test_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_metrics_df.to_csv('../datasets/intermediary/sentences_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract FLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fls(doc,date):\n",
    "    #performs rule-based-matching\n",
    "    #takes a string and a year as input and returns individual sentences that contain a match\n",
    "    \n",
    "    # Load the spaCy language model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    #initialize as nlp object and prepare list for storing matches\n",
    "    \n",
    "    nlp_doc = nlp(doc)\n",
    "    fls=[]\n",
    "    \n",
    "    #two seperate matchers, so that pattern 3 can be checked separately \n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher2 = Matcher(nlp.vocab)\n",
    "    pattern1 =  [{\"TEXT\": {\"IN\": [\"next\", \"subsequent\", \"following\", \"upcoming\", \"incoming\", \"coming\", \"succeeding\", \"carryforward\"]}},\n",
    "            {\"TEXT\": {\"IN\": [\"month\", \"quarter\", \"year\", \"fiscal\", \"taxable\", \"period\"]}}]\n",
    "    pattern2 = [{\"LEMMA\": {\"IN\": [\"aim\", \"anticipate\", \"assume\", \"commit\", \"estimate\", \"expect\", \"forecast\", \"foresee\", \"hope\", \"intend\", \"plan\", \"predict\", \"project\", \"seek\",\"target\"]},\"POS\": \"VERB\"}]\n",
    "    pattern3 = [{\"TEXT\": {\"REGEX\": \"[1-2][0-9][0-9][0-9]\"}, \"LENGTH\": 4}]\n",
    "    matcher.add('pattern1',[pattern1])\n",
    "    matcher.add('pattern2',[pattern2])\n",
    "    matcher2.add('pattern3',[pattern3])\n",
    "    \n",
    "    #if patterns were found in a sentence, append it to the list\n",
    "    for sen in nlp_doc.sents:\n",
    "        if matcher(sen) != []:\n",
    "            fls.append(sen.text)\n",
    "            continue\n",
    "            \n",
    "        #if no matches for patterns 1 or 2 are found, check matches for pattern 3 and check if they are higher than the year provided as input.\n",
    "        elif matcher2(sen) != []:\n",
    "            years=[]\n",
    "            for match_id, start, end in matcher2(sen):\n",
    "                years.append(int(sen[start:end].text))\n",
    "            if max(years) > date:\n",
    "                fls.append(sen.text)\n",
    "                \n",
    "    return fls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "# Define a function for processing a single data point\n",
    "def process_data(data_point,batch_size):\n",
    "    extract_sentences_with_metrics(data_point)\n",
    "    pass\n",
    "\n",
    "\n",
    "# Split your dataset into smaller batches\n",
    "data_batches = [data[i:i+batch_size] for i in range(0, len(data), batch_size)]\n",
    "\n",
    "# Process data points in parallel\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    results = list(executor.map(process_data, data_batches))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Khanh Nguyen\\thesis\\thesis\\extract_sentences.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Khanh%20Nguyen/thesis/thesis/extract_sentences.ipynb#X13sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Iterate through each sentence\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Khanh%20Nguyen/thesis/thesis/extract_sentences.ipynb#X13sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sentences:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Khanh%20Nguyen/thesis/thesis/extract_sentences.ipynb#X13sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     doc \u001b[39m=\u001b[39m nlp(sentence)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Khanh%20Nguyen/thesis/thesis/extract_sentences.ipynb#X13sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     matches \u001b[39m=\u001b[39m matcher(doc)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Khanh%20Nguyen/thesis/thesis/extract_sentences.ipynb#X13sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39m# Check if there are any matches\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Khanh Nguyen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\language.py:1042\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1040\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[0;32m   1041\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1042\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcomponent_cfg\u001b[39m.\u001b[39mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1044\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1045\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test with test_content \n",
    "# Define rules for matcher\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import pandas as pd\n",
    "\n",
    "# Load the spaCy language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define custom spaCy rules using Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"revenue_match\", [[{'LOWER': {'in': ['income', 'proceeds', 'takings', 'receipts', 'sales', 'turnover']}}]])\n",
    "matcher.add(\"net_income_match\", [[{'LOWER': {'in': ['profit', 'earnings', 'loss', 'bottom line', 'net income', 'net loss', 'net profit', 'net earnings']}}]])\n",
    "matcher.add(\"ebit_match\", [[{'LOWER': {'in': ['ebit', 'earnings before interest and taxes', 'operating profit', 'operating income']}}]])\n",
    "matcher.add(\"eps_match\", [[{'LOWER': {'in': ['eps', 'earnings per share']}}]])\n",
    "# Initialize an empty list to store matching sentences\n",
    "matching_sentences = []\n",
    "\n",
    "# Iterate through each column of the DataFrame\n",
    "for column in test_content.columns:\n",
    "    # Iterate through each row in the column\n",
    "    for idx, text in enumerate(test_content[column]):\n",
    "        # Split the text into sentences\n",
    "        sentences = [sentence.text for sentence in nlp(text).sents]\n",
    "        \n",
    "        # Iterate through each sentence\n",
    "        for sentence in sentences:\n",
    "            doc = nlp(sentence)\n",
    "            matches = matcher(doc)\n",
    "            \n",
    "            # Check if there are any matches\n",
    "            if matches:\n",
    "                # Check if 'income' is in the sentence\n",
    "                if 'net income' in sentence.lower():\n",
    "                    # Consider 'net income' as a pattern\n",
    "                    matching_sentences.append({'Column': column, 'Sentence': sentence, 'metric': 'net income'})\n",
    "                else:\n",
    "                    # Consider revenue as a pattern\n",
    "                    matching_sentences.append({'Column': column, 'Sentence': sentence, 'metric': 'revenue'})\n",
    "                \n",
    "\n",
    "# Create a new DataFrame with matching sentences\n",
    "result_df = pd.DataFrame(matching_sentences)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(result_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

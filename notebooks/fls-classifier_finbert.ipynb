{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenk/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/nguyenk/.local/lib/python3.8/site-packages/scipy/__init__.py:143: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 1.17.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 GPU(s) available.\n",
      "We will use the GPU: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 294,202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load train data\n",
    "df = pd.read_csv('../datasets/train.csv')\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some variables\n",
    "random_state = 42\n",
    "\n",
    "\n",
    "# Split data into train and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_text, validation_text, train_labels, validation_labels = train_test_split(df['Sentence'], df['Label'], \n",
    "                                                            random_state=random_state, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-pretrain', do_lower_case=True)\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to their word IDs.\n",
    "train_text_encoded = [tokenizer.encode(sent, \n",
    "                                 add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "                                 padding='max_length', # Pad & truncate all sentences.\n",
    "                                 truncation=True,\n",
    "                                 max_length=512,\n",
    "                                 ) for sent in train_text]\n",
    "validation_text_encoded = [tokenizer.encode(sent,\n",
    "                                    add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "                                    padding='max_length', # Pad & truncate all sentences.\n",
    "                                    truncation=True,\n",
    "                                    max_length=512,\n",
    "                                    ) for sent in validation_text]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the data is extremely imbalanced, we will use SMOTE to balance the data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=random_state, sampling_strategy='auto')\n",
    "train_text_resampled, train_labels_resampled = sm.fit_resample(train_text_encoded, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validation data to tensors\n",
    "train_text_tensor = torch.tensor(train_text_resampled).to(device)\n",
    "train_labels_tensor = torch.tensor(train_labels_resampled).to(device)\n",
    "\n",
    "validation_text_tensor = torch.tensor(validation_text_encoded).to(device)\n",
    "validation_labels_tensor = torch.tensor(validation_labels.values).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validation data to tensors\n",
    "train_text_tensor = torch.tensor(train_text_resampled).to(device)\n",
    "train_labels_tensor = torch.tensor(train_labels_resampled).to(device)\n",
    "\n",
    "validation_text_tensor = torch.tensor(validation_text_encoded).to(device)\n",
    "validation_labels_tensor = torch.tensor(validation_labels.values).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_text_tensor, train_labels_tensor)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_text_tensor, validation_labels_tensor)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-pretrain', num_labels=2,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenk/.local/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14591 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "100%|██████████| 14591/14591 [3:38:40<00:00,  1.11it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "summary results\n",
      "epoch | train loss | train time\n",
      "    1 | 0.03720 |  13120.943731069565\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 920/920 [04:29<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.99\n",
      "  Best model found! Saving it.\n",
      "epoch:  2\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14591/14591 [3:31:28<00:00,  1.15it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "summary results\n",
      "epoch | train loss | train time\n",
      "    2 | 0.21748 |  12688.572664737701\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 920/920 [04:27<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.12\n",
      "epoch:  3\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14591/14591 [3:31:16<00:00,  1.15it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "summary results\n",
      "epoch | train loss | train time\n",
      "    3 | 0.69448 |  12676.38683795929\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 920/920 [04:27<00:00,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the best validation accuracy.\n",
    "best_validation_accuracy = 0.0\n",
    "\n",
    "model.to(device) # send the model to GPU\n",
    "model.train() # switch to train mode i.e. forward, backward, optimization\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5) # choose an optimizer for the gradient descent\n",
    "loss_values = [] # accumulate the losses, can be used with a validation set to choose the epochs so as to avoid overfitting\n",
    "\n",
    "# define number of epochs\n",
    "epochs = 3\n",
    "for epoch in range(epochs): #number of epochs i.e. how many times is the whole dataset passed through the architecture\n",
    "      # =================================\n",
    "      #              Training\n",
    "      # =================================\n",
    "      \n",
    "      print(\"epoch: \", epoch+1)\n",
    "      print(\"Training...\")\n",
    "      # capture time\n",
    "      total_t0 = time.time()\n",
    "      train_total_loss = 0\n",
    "      for batch in tqdm(train_dataloader): # split into batches to fit into the memory\n",
    "            input_ids, labels = batch\n",
    "            input_ids.to(device)\n",
    "            labels.to(device)\n",
    "            \n",
    "            # Always clear any previously calculated gradients before performing a\n",
    "            # backward pass. \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # This will return the loss (rather than the model output) because we\n",
    "            # have provided the `labels`.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(input_ids,labels=labels)\n",
    "            # Calculate the loss i.e. distance between predicted labels and true labels using cross entropy\n",
    "            loss = outputs[0]\n",
    "\n",
    "            # Accumulate the training loss over all of the batches so that we can\n",
    "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "            # single value; the `.item()` function just returns the Python value \n",
    "            # from the tensor.\n",
    "            train_total_loss += loss.item()\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters using the optimizer and the gradient values\n",
    "            optimizer.step()\n",
    "      \n",
    "      # print result summaries\n",
    "      print(\"\")\n",
    "      print(\"summary results\")\n",
    "      print(\"epoch | train loss | train time\")\n",
    "      \n",
    "      # Calculate the average loss over the training data.\n",
    "      avg_train_loss = train_total_loss / len(train_dataloader)\n",
    "      \n",
    "      # Store the loss value for plotting the learning curve.\n",
    "      loss_values.append(avg_train_loss)\n",
    "      \n",
    "      \n",
    "      # training time end\n",
    "      training_time = time.time() - total_t0\n",
    "      print(f\"{epoch+1:5d} | {avg_train_loss:.5f} |  {training_time:}\")\n",
    "      \n",
    "      # =================================\n",
    "      #             Validation\n",
    "      # =================================\n",
    "      # After the completion of each training epoch, measure our performance on\n",
    "      # our validation set.\n",
    "      print(\"\")\n",
    "      print(\"Running Validation...\")\n",
    "      # capture time\n",
    "      total_t0 = time.time()\n",
    "      # switch to evaluation mode i.e. no backward pass\n",
    "      model.eval()\n",
    "      # Tracking variables\n",
    "      \n",
    "      # Evaluate data for one epoch\n",
    "      with torch.no_grad():\n",
    "            preds_list = []\n",
    "            accuracy_list = []\n",
    "            labelsset=[]\n",
    "            accuracy_sum = 0\n",
    "            for batch in tqdm(validation_dataloader):\n",
    "                  input_ids, labels = batch\n",
    "                  input_ids.to(device)\n",
    "                  outputs = model(input_ids)\n",
    "                  logits =outputs.logits.detach().cpu().numpy()   # Taking the softmax of output\n",
    "                  pred=np.argmax(logits, axis=1).tolist()\n",
    "                  acc=accuracy_score(labels.detach().cpu().numpy().tolist(), pred)\n",
    "                  accuracy_sum+=acc\n",
    "                  preds_list.extend(pred)\n",
    "                  accuracy_list.append(acc)\n",
    "                  labelsset.extend(labels.detach().cpu().numpy())\n",
    "      \n",
    "      mean_accuracy = accuracy_sum / len(validation_dataloader)\n",
    "      print(\"  Accuracy: {0:.2f}\".format(mean_accuracy))\n",
    "      \n",
    "      # Check if the current model is the best one and save it.\n",
    "      if mean_accuracy > best_validation_accuracy:\n",
    "            print(\"  Best model found! Saving it.\")\n",
    "            # Save the best model in the specified location.\n",
    "            model.save_pretrained('../model/finbert-fls')\n",
    "            best_validation_accuracy = mean_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "#model.save_pretrained('../model/finbert-fls')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

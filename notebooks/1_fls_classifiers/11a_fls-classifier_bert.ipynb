{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify FLS with DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements the classification model to classify FLS with DistilBERT. The data is the output from the notebook ``01_extract_sentences``. The outline of this notebook is as follows: \n",
    "\n",
    "**Step 1:** Prepare datasets by tokenization, balancing, converting to tensors as well as load and iterate through data in batches. \n",
    "\n",
    "**Step 2:** Train FLS classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenk/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/nguyenk/.local/lib/python3.8/site-packages/scipy/__init__.py:143: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 1.17.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 GPU(s) available.\n",
      "We will use the GPU: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():        \n",
    "    # Tell PyTorch to use the GPU    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 294,202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load train data\n",
    "df = pd.read_csv('../../data/01_interim/train.csv')\n",
    "\n",
    "# Report the number of sentences\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some variables\n",
    "random_state = 42\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_text, validation_text, train_labels, validation_labels = train_test_split(df['Sentence'], df['Label'], \n",
    "                                                            random_state=random_state, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data: 264781 sentences\n",
      "Number of validation data: 29421 sentences\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training data: {len(train_text)} sentences\")\n",
    "print(f\"Number of validation data: {len(validation_text)} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to their word IDs\n",
    "train_text_encoded = [tokenizer.encode(sent, \n",
    "                                 add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "                                 padding='max_length', # Pad & truncate all sentences to set max length.\n",
    "                                 truncation=True,\n",
    "                                 max_length=512, # max length is set to 512 (max length of BERT model) to capture as much information as possible\n",
    "                                 ) for sent in train_text]\n",
    "validation_text_encoded = [tokenizer.encode(sent,\n",
    "                                    add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "                                    padding='max_length', # Pad & truncate all sentences.\n",
    "                                    truncation=True,\n",
    "                                    max_length=512,\n",
    "                                    ) for sent in validation_text]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the data is extremely imbalanced, we will use SMOTE to balance the data\n",
    "sm = SMOTE(random_state=random_state, sampling_strategy='auto')\n",
    "train_text_resampled, train_labels_resampled = sm.fit_resample(train_text_encoded, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of train data after resampling: 466892\n",
      "Distribution of labels after resampling: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    233446\n",
       "0    233446\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Total length of train data after resampling: {len(train_text_resampled)}\")\n",
    "print(\"Distribution of labels after resampling: \\n\")\n",
    "train_labels_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to tensors and load data in DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the training and validation data into tensors, which are multi-dimensional arrays that can be processed by a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validation data to tensors\n",
    "train_text_tensor = torch.tensor(train_text_resampled).to(device)\n",
    "train_labels_tensor = torch.tensor(train_labels_resampled).to(device)\n",
    "\n",
    "validation_text_tensor = torch.tensor(validation_text_encoded).to(device)\n",
    "validation_labels_tensor = torch.tensor(validation_labels.values).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the batch size for training and validation data. The batch size determines the number of samples that will be processed together in each iteration during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_text_tensor, train_labels_tensor)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "validation_data = TensorDataset(validation_text_tensor, validation_labels_tensor)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenk/.local/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14591 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "100%|██████████| 14591/14591 [1:54:06<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "summary results\n",
      "epoch | train loss | train time\n",
      "    1 | 0.02923 |  6846.694295167923\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 920/920 [02:18<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.99\n",
      "epoch:  2\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14591/14591 [1:49:41<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "summary results\n",
      "epoch | train loss | train time\n",
      "    2 | 0.01572 |  6581.928730726242\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 920/920 [02:18<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.99\n",
      "epoch:  3\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14591/14591 [1:49:34<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "summary results\n",
      "epoch | train loss | train time\n",
      "    3 | 0.01384 |  6574.551898956299\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 920/920 [02:18<00:00,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the best validation accuracy.\n",
    "best_validation_accuracy = 0.0\n",
    "\n",
    "model.to(device) # send the model to GPU\n",
    "model.train() # switch to train mode i.e. forward, backward, optimization\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5) # choose an optimizer for the gradient descent\n",
    "loss_values = [] # accumulate the losses, can be used with a validation set to choose the epochs so as to avoid overfitting\n",
    "\n",
    "# define number of epochs\n",
    "epochs = 3\n",
    "for epoch in range(epochs): #number of epochs i.e. how many times is the whole dataset passed through the architecture\n",
    "      # =================================\n",
    "      #              Training\n",
    "      # =================================\n",
    "      \n",
    "      print(\"epoch: \", epoch+1)\n",
    "      print(\"Training...\")\n",
    "      # capture time\n",
    "      total_t0 = time.time()\n",
    "      train_total_loss = 0\n",
    "      for batch in tqdm(train_dataloader): # split into batches to fit into the memory\n",
    "            input_ids, labels = batch\n",
    "            input_ids.to(device)\n",
    "            labels.to(device)\n",
    "            \n",
    "            # Always clear any previously calculated gradients before performing a\n",
    "            # backward pass. \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # This will return the loss (rather than the model output) because we\n",
    "            # have provided the `labels`.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(input_ids,labels=labels)\n",
    "            # Calculate the loss i.e. distance between predicted labels and true labels using cross entropy\n",
    "            loss = outputs[0]\n",
    "\n",
    "            # Accumulate the training loss over all of the batches so that we can\n",
    "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "            # single value; the `.item()` function just returns the Python value \n",
    "            # from the tensor.\n",
    "            train_total_loss += loss.item()\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters using the optimizer and the gradient values\n",
    "            optimizer.step()\n",
    "      \n",
    "      # print result summaries\n",
    "      print(\"\")\n",
    "      print(\"summary results\")\n",
    "      print(\"epoch | train loss | train time\")\n",
    "      \n",
    "      # Calculate the average loss over the training data.\n",
    "      avg_train_loss = train_total_loss / len(train_dataloader)\n",
    "      \n",
    "      # Store the loss value for plotting the learning curve.\n",
    "      loss_values.append(avg_train_loss)\n",
    "      \n",
    "      \n",
    "      # training time end\n",
    "      training_time = time.time() - total_t0\n",
    "      print(f\"{epoch+1:5d} | {avg_train_loss:.5f} |  {training_time:}\")\n",
    "      \n",
    "      # =================================\n",
    "      #             Validation\n",
    "      # =================================\n",
    "      # After the completion of each training epoch, measure our performance on\n",
    "      # our validation set.\n",
    "      print(\"\")\n",
    "      print(\"Running Validation...\")\n",
    "      # capture time\n",
    "      total_t0 = time.time()\n",
    "      # switch to evaluation mode i.e. no backward pass\n",
    "      model.eval()\n",
    "      # Tracking variables\n",
    "      \n",
    "      # Evaluate data for one epoch\n",
    "      with torch.no_grad():\n",
    "            preds_list = []\n",
    "            accuracy_list = []\n",
    "            labelsset=[]\n",
    "            accuracy_sum = 0\n",
    "            for batch in tqdm(validation_dataloader):\n",
    "                  input_ids, labels = batch\n",
    "                  input_ids.to(device)\n",
    "                  outputs = model(input_ids)\n",
    "                  logits =outputs.logits.detach().cpu().numpy()   # Taking the softmax of output\n",
    "                  pred=np.argmax(logits, axis=1).tolist()\n",
    "                  acc=accuracy_score(labels.detach().cpu().numpy().tolist(), pred)\n",
    "                  accuracy_sum+=acc\n",
    "                  preds_list.extend(pred)\n",
    "                  accuracy_list.append(acc)\n",
    "                  labelsset.extend(labels.detach().cpu().numpy())\n",
    "      \n",
    "      mean_accuracy = accuracy_sum / len(validation_dataloader)\n",
    "      print(\"  Accuracy: {0:.2f}\".format(mean_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "# At the time this model was trained, I did not save it directly to the HuggingFace repository but to a local folder. I then uploaded this to the website. \n",
    "model.save_pretrained('../../models/distilbert-fls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier is evaluated along with the FLS classifier based on FinBERT (notebook ``11b_fls-classifier_finbert``). The name of the evaluation notebook is ``12_evaluate_fls_classifiers``."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

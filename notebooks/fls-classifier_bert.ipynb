{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 235,403\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load train data\n",
    "df = pd.read_csv('../datasets/train_data.csv')\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some variables\n",
    "random_state = 42\n",
    "\n",
    "\n",
    "# Split data into train and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_text, validation_text, train_labels, validation_labels = train_test_split(df['Sentence'], df['Label'], \n",
    "                                                            random_state=random_state, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Our principal sources of liquidity are cash from operations and more recently, proceeds from our debt and equity offerings.\n",
      "Token IDs: [101, 1996, 10768, 11890, 3843, 1037, 8001, 3343, 4861, 5517, 2008, 2009, 2097, 2053, 2936, 9146, 13117, 2015, 4114, 2004, 19875, 4523, 2000, 8980, 2019, 3318, 4171, 21447, 1999, 2037, 3465, 1011, 1997, 1011, 2326, 6165, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to their word IDs.\n",
    "train_text_encoded = [tokenizer.encode(sent, \n",
    "                                 add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "                                 padding='max_length', # Pad & truncate all sentences.\n",
    "                                 truncation=True,\n",
    "                                 max_length=512,\n",
    "                                 ) for sent in train_text]\n",
    "validation_text_encoded = [tokenizer.encode(sent,\n",
    "                                    add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "                                    padding='max_length', # Pad & truncate all sentences.\n",
    "                                    truncation=True,\n",
    "                                    max_length=512,\n",
    "                                    ) for sent in validation_text]\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', train_text[0])\n",
    "print('Token IDs:', train_text_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the data is extremely imbalanced, we will use SMOTE to balance the data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=random_state, sampling_strategy=0.5)\n",
    "train_text_resampled, train_labels_resampled = sm.fit_resample(train_text_encoded, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validation data to tensors\n",
    "train_text_tensor = torch.tensor(train_text_resampled).to(device)\n",
    "validation_text_tensor = torch.tensor(validation_text_encoded).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>batch_size = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">32</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 # Create the DataLoader for our training set.</span>                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 5 train_data = TensorDataset(train_text_tensor, train_labels_resampled)                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 # Create the DataLoader for our validation set.</span>                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'train_text_tensor'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m5\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 2 \u001b[0mbatch_size = \u001b[94m32\u001b[0m                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 3 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0m\u001b[2m# Create the DataLoader for our training set.\u001b[0m                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 5 train_data = TensorDataset(train_text_tensor, train_labels_resampled)                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0mtrain_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=\u001b[94mTrue\u001b[0m)              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[2m# Create the DataLoader for our validation set.\u001b[0m                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'train_text_tensor'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_text_tensor, train_labels_resampled)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_text_tensor, validation_labels)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train FLS Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2,output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>  1 model.to(device) <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># send the model to GPU</span>                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  2 </span>model.train() <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># switch to train mode i.e. forward, backward, optimization</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  3 </span>optimizer = AdamW(model.parameters(), lr=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5e-5</span>) <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># choose an optimizer for the gradient de</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  4 </span>loss_values = [] <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># accumulate the losses, can be used with a validation set to choose th</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'device'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m  1 model.to(device) \u001b[2m# send the model to GPU\u001b[0m                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  2 \u001b[0mmodel.train() \u001b[2m# switch to train mode i.e. forward, backward, optimization\u001b[0m                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  3 \u001b[0moptimizer = AdamW(model.parameters(), lr=\u001b[94m5e-5\u001b[0m) \u001b[2m# choose an optimizer for the gradient de\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  4 \u001b[0mloss_values = [] \u001b[2m# accumulate the losses, can be used with a validation set to choose th\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'device'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.to(device) # send the model to GPU\n",
    "model.train() # switch to train mode i.e. forward, backward, optimization\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5) # choose an optimizer for the gradient descent\n",
    "loss_values = [] # accumulate the losses, can be used with a validation set to choose the epochs so as to avoid overfitting\n",
    "\n",
    "# define number of epochs\n",
    "epochs = 3\n",
    "for epoch in range(epochs): #number of epochs i.e. how many times is the whole dataset passed through the architecture\n",
    "      # =================================\n",
    "      #              Training\n",
    "      # =================================\n",
    "      \n",
    "      print(\"epoch: \", epoch+1)\n",
    "      print(\"Training...\")\n",
    "      # capture time\n",
    "      total_t0 = time.time()\n",
    "      train_total_loss = 0\n",
    "      for batch in tqdm(train_dataloader): # split into batches to fit into the memory\n",
    "            input_ids, labels = batch\n",
    "            input_ids.to(device)\n",
    "            labels.to(device)\n",
    "            \n",
    "            # Always clear any previously calculated gradients before performing a\n",
    "            # backward pass. \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # This will return the loss (rather than the model output) because we\n",
    "            # have provided the `labels`.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(input_ids,labels=labels)\n",
    "            # Calculate the loss i.e. distance between predicted labels and true labels using cross entropy\n",
    "            loss = outputs[0]\n",
    "\n",
    "            # Accumulate the training loss over all of the batches so that we can\n",
    "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "            # single value; the `.item()` function just returns the Python value \n",
    "            # from the tensor.\n",
    "            train_total_loss += loss.item()\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters using the optimizer and the gradient values\n",
    "            optimizer.step()\n",
    "      \n",
    "      # print result summaries\n",
    "      print(\"\")\n",
    "      print(\"summary results\")\n",
    "      print(\"epoch | train loss | train time\")\n",
    "      \n",
    "      # Calculate the average loss over the training data.\n",
    "      avg_train_loss = train_total_loss / len(train_dataloader)\n",
    "      \n",
    "      # Store the loss value for plotting the learning curve.\n",
    "      loss_values.append(avg_train_loss)\n",
    "      \n",
    "      \n",
    "      # training time end\n",
    "      training_time = time.time() - total_t0\n",
    "      print(f\"{epoch+1:5d} | {avg_train_loss:.5f} |  {training_time:}\")\n",
    "      \n",
    "      # =================================\n",
    "      #             Validation\n",
    "      # =================================\n",
    "      # After the completion of each training epoch, measure our performance on\n",
    "      # our validation set.\n",
    "      print(\"\")\n",
    "      print(\"Running Validation...\")\n",
    "      # capture time\n",
    "      total_t0 = time.time()\n",
    "      # switch to evaluation mode i.e. no backward pass\n",
    "      model.eval()\n",
    "      # Tracking variables\n",
    "      \n",
    "      # Evaluate data for one epoch\n",
    "      with torch.no_grad():\n",
    "            preds_list = []\n",
    "            accuracy_list = []\n",
    "            labelsset=[]\n",
    "            accuracy_sum = 0\n",
    "            for batch in tqdm(validation_dataloader):\n",
    "                  input_ids, labels = batch\n",
    "                  input_ids.to(device)\n",
    "                  outputs = model(input_ids)\n",
    "                  logits =outputs.logits.detach().cpu().numpy()   # Taking the softmax of output\n",
    "                  pred=np.argmax(logits, axis=1).tolist()\n",
    "                  acc=accuracy_score(labels.detach().cpu().numpy().tolist(), pred)\n",
    "                  accuracy_sum+=acc\n",
    "                  preds_list.extend(pred)\n",
    "                  accuracy_list.append(acc)\n",
    "                  labelsset.extend(labels.detach().cpu().numpy())\n",
    "      \n",
    "      mean_accuracy = accuracy_sum / len(validation_dataloader)\n",
    "      print(\"  Accuracy: {0:.2f}\".format(mean_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import test set\n",
    "df_test = pd.read_csv('../datasets/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to their word IDs.\n",
    "text_encoded = [tokenizer.encode(sent,\n",
    "                                add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "                                padding='max_length', # Pad & truncate all sentences.\n",
    "                                truncation=True,\n",
    "                                max_length=512,\n",
    "                                ) for sent in df_test['Sentence']]\n",
    "\n",
    "# Data to tensors\n",
    "text_tensor = torch.tensor(train_text_resampled).to(device)\n",
    "\n",
    "# Create the DataLoader for our test set\n",
    "test_data = TensorDataset(text_tensor)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_dataloader):\n",
    "# Put the model in evaluation mode--the dropout layers behave differently\n",
    "# during evaluation.  \n",
    "  model.eval()\n",
    "# Telling the model not to compute or store gradients, saving memory and\n",
    "# speeding up the process\n",
    "  with torch.no_grad():\n",
    "    preds_list = []\n",
    "    accuracy_list = []\n",
    "    labelsset=[]\n",
    "    accuracy_sum = 0\n",
    "    for batch in tqdm(test_dataloader):\n",
    "      input_ids, labels = batch\n",
    "      input_ids.to(device)\n",
    "      outputs = model(input_ids)\n",
    "      logits =outputs.logits.detach().cpu().numpy()   # Taking the softmax of output\n",
    "      pred=np.argmax(logits, axis=1).tolist()\n",
    "      acc=accuracy_score(labels.detach().cpu().numpy().tolist(), pred)\n",
    "      accuracy_sum+=acc\n",
    "      preds_list.extend(pred)\n",
    "      accuracy_list.append(acc)\n",
    "      labelsset.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "  mean_accuracy = accuracy_sum / len(test_dataloader)\n",
    "  return mean_accuracy, preds_list, accuracy_list, labelsset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy, preds_list, accuracy_list, labelsset = test(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on test set: \", mean_accuracy)\n",
    "print(\"Classification report: \", classification_report(labelsset,preds_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
